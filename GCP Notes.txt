GCP

- Compute and store are seperated for better resource utilization
- Speed of transfer of data between compute and store - Petabyte bisectional bandwidth
- gsutil - command prompt
- serverless
- Stackdriver for monitoring the activities from logs
- DataEngineer - Build Pipelines
- 5 Dimension of Data
	- Validity
	- Accuracy
	- Completeness
	- Consistency
	- Uniformity
- While quering data the most of the time consumption is for I/O operations rather than the computes involved in it. That is reducing the query to only pull specific Columns information runs faster than the all the columns.
	-Avoid self-joins of large tables
	-Efficient joins - Denormalization
	-Use window functions instead of Joins
	- Join with precomputed values
	-Approximate aggregation functions 
		-  COUNT(DISTINCT …), we can use APPROX_COUNT_DISTINCT
		- APPROX_QUANTILES to compute percentiles
		- APPROX_TOP_COUNT to find the top elements 
		- APPROX_TOP_SUM to compute top elements based on the sum of an element.


- wget http://www.ssa.gov/OACT/babynames/names.zip - used to get files form other locations

Levels
Level-1 - Security
Level-2 - Compute - Storage - Network
Level-3 - Big Data and ML Products


GCP Structure
Organization - TCS
Folders - Dep1,Dep2 -> Team1, Team2
Projects - Test, Dev
Resources - Compute

IAM - access control
ACL - Access control List - multiple accounts
Service Accounts creation
Encryption keys
Signed URL - To provide a Cloud storage URL for access and restricting the URL with respect the access time. can only be used with gsutil - No UI Console



Compute
-------------
- Compute Engine - VM's
- App Engine - Can run application Code
- Kubernities Engine - Run Containers
- Cloud Function - Logic for an event
- Cloud Dataproc - can run Apache Spark jobs, Dataflows - ML Models, support both batch and streaming data 
- Pub/Sib - Publish and Subscriptions for events.
- Cloud DataFlow - Apache Beam jobs
- Cloud Data Fusion - Build Pipeline by drag and drop and can run in Clound Dataproc
- Cloud Monitoring - can be used to monitor any process and create alerts and dashboards


Compute Engine:
 - Standard VM's - can spin up by specifying region and zones, when required has some store on cluster
 - Premptible VM's - can be used for quick computing, no cluster store
 - Persistance Disks - to create storage space with X storage
	Data persist even if the VM is shutdown 
	Data is restricted to the Zone it exist so any VM which is in other region or zone cant access this.
	Snapshot are avaiable globally and can be used to create disk in other zones as needed
	SSD - has faster IOP
	increase of storage space - increases the IOP

Cloud DataProc
- to Apachi Spark or Hadoop jobs 
- Uses
	- Persistent Clusters - Persistent data cluster
	- Ephemeral Clusters - non Persistent Data Cluster - Prefer to use them 
- On-permis - HDFS to Cloud Storage 
- Seperate Compute from storate 
	- Run hadoop jobs on Cluster with memory block size - inefficient - old way - On-permis
	- Run Hadoop jobs on Dataproc with Data from Cloud Storage - Petabite bandwidth
- Googles Data Center
	- Store - Colossus
	- Network - Jupiter
- local HDFS is better if it is small chuncks of data, lot many change in the data.
- Cloud Storage for input and output.
- Autoscaling is reactive - based on cluster utilization.

Cloud Data Fusion:
- to create visual data pipelines
	Wrangler - for reviewing the data, create transformations and create a reciepie which can be used in datafusion to run the recipie on teh whole dataset
	Data Pipeline - Framework - drap and drop 
	Rules Engine - for rules from Business
	Metadata Aggregator - field level details
- Cloud Data Fusion provisions an ephemeral Cloud Dataproc cluster, runs the pipeline
-Lineage of data - transformations which happened on each field of data from source to destination

Cloud Composer:
- orchestrator built on open source Apache Air Flow to streamline multiple activites.
- Need to create an environment
- Each of the block of operations are created in DAG(Direct Acyclic Graph) - takes input and outputs but cannot trigger itself
	- a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.
	- written in Python language and placed in DAG folder
	- run this python code to create a DAG object with tasks
	- Each Task with have inbuild operators 
- can be trigger by 
	- Push - Event - using Cloud Functions
	- Pull - Schedule
- monitoring can be done using stackdriver or cloud logging

Cloud Dataflow:
- based on Apache Beam = Batch + stEAM
- same code for can be used for Batch and stream processing of data 
- new datapipeline - build for future
- Contains - Parallel execution
	- pCollection - immutable data - copies of data are made between transformations
		- Unbounded pCollection - Streaming Data
		- bounded pCollection - Batch Data
	- pTransform - Input/Transformations/output - Map,FlatMap,ParDo,GroupByKey,coGroupByKey,Combine,CombineFn,Flatten,Partition
	- prunners - compute power - local/cloud
	- Pipeline
- Contains the data in pCollections as byte stream(elements) rather than fixed datatypes
- AutoScaling - AutoBalancing in between the process - Auto Optimization once the pipeline is submitted. If there are independent transforms, they would process parallelly
- Intellegent Watermark
- Side Inputs
- Windows
- data skewing - holding up a resource for other resources to complete the work

Cloud Pub/Sub 
- Components
	- Publisher - publishes messages based on Topics created
	- Subscriber - Subscribes to the Topics
- one to many mapping between publisher -> Topic -> Subscription -> Subscriber
- Request Types
	- Pull - Subscribers initiaties the actions, requests messages, pub sends the message, sub acknowledges the message
	- Push - Publisher pushes the message to Subscriber, Sub Acknowledges the message
- Message are sent to Subscriber for if there is delay with acknowledgement time.
- Issues
	- Messages can arrive not in order of the event
	- Duplication of message expected
	- Publisher stores for only 7 days.
	- each message has a limitation of 10MB and stored in binary order
Commands

gcloud pubsub topics create sandiego
gcloud pubsub topics publish sandiego --message "hello"
messageIds:
- '2201523698972039'
gcloud pubsub subscriptions create --topic sandiego mySub1
Created subscription [projects/qwiklabs-gcp-02-447c7e5c4d12/subscriptions/mySub1].
gcloud pubsub subscriptions pull --auto-ack mySub1
Listed 0 items.
gcloud pubsub topics publish sandiego --message "hello again"
messageIds:
- '2201516210054304'
gcloud pubsub subscriptions pull --auto-ack mySub1
┌─────────────┬──────────────────┬────────────┐
│     DATA    │    MESSAGE_ID    │ ATTRIBUTES │
├─────────────┼──────────────────┼────────────┤
│ hello again │ 2201516210054304 │            │
└─────────────┴──────────────────┴────────────┘
gcloud pubsub subscriptions delete mySub1

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
Storage
---------
DataLake - Single source of data of the organization - gets data from multiple sources in multiple formats - Datadump - Cloud Storage
Datawarehouse - Processed data which can be used for analytics - Known data - Cloud BigQuery

CLASS
- Standard - Daily or in usage
- Nearline - Once in a week
- Coldline - Once in a Month
- Archive - High retrival Cost, Low Storage cost - once in a year

Cloud Storage - Buckets - files
Cloud SQL - RDMS - single Database/Location - Supports MYSQL/POSTGRES/SQLSERVER, can be single or multiple Zone(Production)
Cloud Spinner - RDMS - Multiple Database/across globe
Cloud Datastore - No SQL Database
Cloud BigTable - High throughput - Sensor data, Optimized for writes
Cloud BigQuery - DataAnalytics with ML SQL, optimized for Reads
Cloud Shell - for quering DB's
Data Catalog - Metadata Management Services

Cloud Storage
 - Same zone - low latency
 - Also called as Object Storage
 - Limitation of the object size to 5T
 - version is allowed but is OFF by default
 - Class is respect to object no with respect to version
 - Small data transfers - console, gutil
 - large data transfers - Transfer applications(one time transfer), Transfer services(scheduled/repeatitive)

Cloud BigQuery
	- can use SQL for quering the data 
	- Managed Database(backup, optimization)
- Count() only returns the number but doesn't remove the duplicates
- Groupby is used to remove duplicates
- explore datastudio - for graphs on data
- Cloud Dataprep - to get insights on the data
- Normalization(multiple tables to reduce duplication of data) is for RDMS but for BigQuery - Single table as you can have Arrays for columns and STRUCTs (column families) datatype - record
- Follows De-Normalization Process by combining multiple table data into a single table using Structs
- Nested and repeated Data
- get granular data from ARRAYs when you need it but not be punished if you don't (BigQuery stores each column individually on disk)
Sample:  Repeat (Datatype)
1	1234-1	start		01-01-2021
		inprogress	
		complete

Event - Record (Mode) 
	Event.start	- Timestamp 
	Event.end	- Timestamp

Struct Sample:
#standardSQL
SELECT race, participants.name
FROM racing.race_results AS r, r.participants

#standardSQL
SELECT COUNT(p.name) AS racer_count
FROM racing.race_results AS r, UNNEST(r.participants) AS p

- Unnested is used to extract the data from a Record Mode so as to flatten the data.
	You need to UNNEST() arrays to bring the array elements back into rows
	UNNEST() always follows the table name in your FROM clause (think of it conceptually like a pre-joined table)
- Slots - unit of computation Power 
- Project.dataset.table - bigquery structure
	- Dataset - Similar to Database
- authentication with IAM - at dataset level
- to give authentication at record level  - create view on the table and provide required access to view.
- Views cannot be exported.
- Optimized for read not good for updates(use BigTable)
- Big Data Transfer Service  -  EL , ELT, ETL
- Partitioning - Save query time
	- Defining Partition on a date related column 
	- can also have the flexibility to expire the data 
	- all_sessions_raw_(N) - number of tables partitioned by date
	- Sample with wildcard SELECT * FROM `ecommerce.sales_by_sku_2017*`
- Clustering - Saves query time and processing time (Similar to Index) - Saved in a single clusters - reduces latency
	- Defining multiple columns are clusters
	- Order of the columns is critical.
	- Bigquery internally Clusters data automatically
- If a query is run, the result is generally cache for 24 hrs, If we run the same query there will not be any processing done and query time = 0 Sec
- Compatable with backward filling - filling any missed values which arrive late
- Lineage - To track the source, process and destination of the data - Metadata of the data 
	- Labels can be used for tracking - Key: Value pairs
- BigQuery SQL can solve data consistency problems(5 dimensions of data)
- Cross Join
#standardSQL
SELECT race, participants.name
FROM racing.race_results
CROSS JOIN
race_results.participants # full STRUCT name

#standardSQL
SELECT race, participants.name
FROM racing.race_results AS r, r.participants
- Some Commands
finding the number of elements with ARRAY_LENGTH(<array>)
deduplicating elements with ARRAY_AGG(DISTINCT <field>)
ordering elements with ARRAY_AGG(<field> ORDER BY <field>)
limiting ARRAY_AGG(<field> LIMIT 5)

bq show bigquery-public-data:samples.shakespeare 

bq query --use_legacy_sql=false \
'SELECT
   word,
   SUM(word_count) AS count
 FROM
   `bigquery-public-data`.samples.shakespeare
 WHERE
   word LIKE "%raisin%"
 GROUP BY
   word'

#standardSQL

# copy one day of ecommerce data to explore
CREATE OR REPLACE TABLE ecommerce.revenue_transactions_20170801
#schema
(
  fullVisitorId STRING NOT NULL OPTIONS(description="Unique visitor ID"),
  visitId STRING NOT NULL OPTIONS(description="ID of the session, not unique across all users"),
  channelGrouping STRING NOT NULL OPTIONS(description="Channel e.g. Direct, Organic, Referral..."),
  totalTransactionRevenue FLOAT64 NOT NULL OPTIONS(description="Revenue for the transaction")
)
 OPTIONS(
   description="Revenue transactions for 08/01/2017"
 ) AS
 SELECT DISTINCT
  fullVisitorId,
  CAST(visitId AS STRING) AS visitId,
  channelGrouping,
  totalTransactionRevenue / 1000000 AS totalTransactionRevenue
 FROM `data-to-insights.ecommerce.all_sessions_raw`
 WHERE date = '20170801'
      AND totalTransactionRevenue IS NOT NULL #XX transactions
;

What are ways to overcome stale data? - reflecting of changes when the upstream table has changes
There are two ways to overcome stale data in reporting tables:
- Periodically refresh the permanent tables by re-running queries that insert in new records. This can be done with BigQuery scheduled queries or with a Cloud Dataprep / Cloud Dataflow workflow.
- Use logical views to re-run a stored query each time the view is selected
	
#standardSQL
CREATE OR REPLACE VIEW ecommerce.vw_latest_transactions
OPTIONS(
  description="latest 100 ecommerce transactions",
  labels=[('report_type','operational')]
)
AS
SELECT DISTINCT
  date,
  fullVisitorId,
  CAST(visitId AS STRING) AS visitId,
  channelGrouping,
  totalTransactionRevenue / 1000000 AS totalTransactionRevenue
 FROM `data-to-insights.ecommerce.all_sessions_raw`
 WHERE totalTransactionRevenue IS NOT NULL
 ORDER BY date DESC # latest transactions
 LIMIT 100
;

- SAFE_DIVIDE(field1,field2) to avoid divide by 0 errors

-  The difference between a UNION and UNION ALL is that a UNION will not include duplicate records.

- Linking external tables to BigQuery  - Importing data from the google sheet will not create the table but would estabilish a connection. if we update the google sheet. the changes are reflected in the query.

 - STRING_AGG(DISTINCT v2ProductName ORDER BY v2ProductName LIMIT 10) AS products_ordered
	- is used to get sample data from a column with comma seperated.

- ARRAY_AGG(DISTINCT v2ProductName) AS push_all_names_into_array
	- is used to create an array of product names
SELECT
  fullVisitorId,
  date,
  ARRAY_AGG(DISTINCT v2ProductName) AS products_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT v2ProductName)) AS distinct_products_viewed,
  ARRAY_AGG(DISTINCT pageTitle) AS pages_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT pageTitle)) AS distinct_pages_viewed
  FROM `data-to-insights.ecommerce.all_sessions`
WHERE visitId = 1501570398
GROUP BY fullVisitorId, date
ORDER BY date

- Joins
	- Cross join - outerjoin - no need of join condition - merges the tables
	- Inner join - by default - only the common data in both tables
	- Left Inner Join - full data of the left table and matching with the second tables - commonly used join.
	- right Inner join - full data of the right table and matching with the second tables - generaly we will use left inner join by swapping tables.
	
-  The summary will be slightly different because the RAND() component of the WHERE clause will return a different 1% selection of the data each time the query is executed.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ML
----------

Artificial Intelligence - Discipine(Like Physics)
	- Machince Learning - one alternative (Like Newton's law)
		- Deep Learning

ML Activities
- Ingest Data (Transfer Services/CloudStorage)
	- Prepare(Dataproc/DataFlow/Dataprep/Bigquery)
		- PreProcess(DataFlow/DataProc/BigQuery)
			- Discover(AI Hub)
				- Develop(Data Labeling Platform/Deep learning VM Image/AI Platform Notebooks)
					- Train(AI Platform Training/Kubeflow- onpermise)
						- Test&Analysis(TFX Tools)
							- Deploy(AI Platform Predication/Kuberflow-onpremise)
- Kubeflow 
	- Google Opensource ML Pipeline framework
	- can be packaged with dependencies and can run on any cloud provider
	- provides visual flow the actvitives
AutoML API - APIs and Services
Vision - Imports -> Train -> Evaluate -> Validate
	- Need labled data

ML Framework
	- Tensor Flow
	- AI Platform
- Models
	- Build a Custom Model
		- Cloud TPU's
		- Kubernetes Engine
		- Cloud AI Plaftform
		- Big Query ML
		- Cloud DataProc
		- Compute Engine		
	- Build Custom Model (Code Less) - bring your data
		- Cloud AutoML 
		- Cloud API Hub
	- Pre-trained ML Model
		- Cloud Vision API
		- Cloud Speech API
		- Cloud Jobs API
		- Cloud Translation API
		- Cloud Natural Language API
		- Cloud Video Intelligence API
		- Cloud Data Loss Prevention API
		- Cloud Speech Synthesis API
		- Cloud Dialogflow

- Training Data -> Auto ML -> Predication with REST API

NLP
 - Syntactical Analysis - Grammer
 - Entity Analysis - cateogorizing words - phone numbers, States
 - Sentiment Analysis - Emotional opinion - +ve,-ve, neutral
	- Score -1 to +1
	- Magnitude = 0 to infinity


AI Platform - cloud Notebooks
- for sharing notebooks 
- Maggic Functions - %%bigquery

BigQuery ML
- Ingest - ML.Weights(inspect)
	- Train - ML.TrainingInfo
		- Evaluate - ML.Evaluate
			- Predict - ML.Predict
- Transform - keyword used in transformations to the raw data before training it and is used to do the same transformations before the prediction
- Labels
- Feature info - ML.Feature_info
- Models
	- Supervised - labeled data
		- Classification
			- linear
			- logistic
			- dnn_classifier
			- boosted_tree_classifier
		- Regression
			- linear_
			- Logistic_reg
			- dnn_regressor
			- boosted_tree_regressor
		- Recommendation
			- tensorflow
			- matrix_factorization
	- UnSupervised - unlabeled data - clusters
		- kmeans

Example:

Model Creations:
CREATE OR REPLACE MODEL `ecommerce.classification_model`
OPTIONS
(
model_type='logistic_reg',
labels = ['will_buy_on_return_visit']
)
AS
<<Select query data>>


Evaluate: - area under the curve
SELECT
  roc_auc,
  CASE
    WHEN roc_auc > .9 THEN 'good'
    WHEN roc_auc > .8 THEN 'fair'
    WHEN roc_auc > .7 THEN 'decent'
    WHEN roc_auc > .6 THEN 'not great'
  ELSE 'poor' END AS model_quality
FROM
  ML.EVALUATE(MODEL ecommerce.classification_model,  ( <<select query data>>)

evaulation params - better model
roc_auc - more value 
SQRT - less value
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

Streaming Data processing:
-------------------------------
- Continous Stream of data, Difficult to handle aggregate functions
- Data consists of 	
	ELEMENT DTS
- DTS is a event Timestamp setup buy pubsub
- Solution for streaming data - WINDOWS
	- Fixed Window - 60 Sec - new window starts every 60 sec
	- sliding Window - (60,5) - New window starts for every 5 secs with a window span of 60 secs
	- Session Window  - (10*60) - session max size - 600 sec
- windows gets flushed out or customized behaviour after the timeout
- Handles latency of data - late data by using Intellegent watermark
	lag time = actual time - expected time.
	each window is extended to end at windowtime + lag time to accomidate the late data
	- WaterMark options
		- AfterWatermark - based on event time triggered (DTS)
		- AfterProcessTime - based on processing time
		- AfterCount - based on Data driven trigger.
- Streaming Data into Bigquery
	- Uses Streaming Buffer - there is a quote and price for loading Streaming data into Bigquery
	- prefer to use batch or repeated batch which is free
	- DataStudio is used to do visual representation of data.
		- Access to Charts in Datastudio doesn't provide access to the underlying data
		- if Edit access is provided to chart, access is provided to the whole table and rows from which the chart is made.


Cloud BigTable 
 - Very High through put with very less latency
- No SQL database
- Cluster based
- Structure
	- Based on Colossus File System to store Data - maintains 3 replicas
	- Uses Clusters with multiple nodes to store metadata of Tablets
	- Colossus uses TABLETS - Datastructure which contains pointers to the data
- learning system and self optimized and self reliabilie and self redistribution of traffic- Min 300GB required
- Fault Tolerant - if a node is lost, a new nodes come-up and only the metadata and pointers are update not the actual data as it lies is colossus
- Data stored in Tables
	- Rows and Columns (Column Families)
	- Only one index - row key (Unique) 
- converts all the searches and Sorts into Scans
	- Construction of the row key for specific scnearios is key
- increase in nodes have linear relation ship with number of operations can be performed
- Visualizer with Heat map to looking for key insights on how the data is being used


GIS Functions
---------------
- used to get geo-spacial details
- Bigquery GeoViz for Graphical view of Geo Spacail points in Maps.
- Structs are used to organize columns from Different Tables for more reability.
- With Clauses are used to create named Sub Queries and used for pre-processing of data rather than creating staging tables.
- Lead and Lag Functions to get the previous value of a column for a  specific keys. Need to use partitions and orderby to have the data in sorted order
- RANK function
