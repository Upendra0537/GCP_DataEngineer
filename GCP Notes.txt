Google Cloud Profession - Data Engineering

- Compute and store are seperated for better resource utilization
- Speed of transfer of data between compute and store - Petabyte bisectional bandwidth
- gsutil - command prompt
- DataEngineer - Build Data Pipelines
- 5 Dimension of Data
	- Validity
	- Accuracy
	- Completeness
	- Consistency
	- Uniformity

- wget http://www.ssa.gov/OACT/babynames/names.zip - used to get files form other locations
- Stackdriver Monitoring is useful for measuring performance metrics and alerts, but not for logs(Stackdriver logging)

Levels
Level-1 - Security
Level-2 - Compute - Storage - Network
Level-3 - Big Data and ML Products

GCP Structure
	Organization - TCS
		Folders - Dep1,Dep2 -> Team1, Team2
			Projects - Test, Dev
				Resources - Compute


Reference Architectures - gcp.solutions

Security
----------
Regualtion Compliance - HIPAA
Shared Responsibilty Model
	- Platform Security Features
	- Google InfoSec Team
	- Customer Following Best Practices - IAM roles
	- Principle of Least previlages - IAM roles
	- Always be watching - logs 

IAM
	- Human User - Username/Password
	- Service Account - Key
	- Cloud IAM API - Oauth/openid/jwt - for auditing
	- Service Account User Role - service account access other service account
	
Roles
- Billing Administrator is the appropriate role to assign to the finance team for visibility of GCP charges
- Billing Account Creator - Create new billing accounts
- Billing Account Administrator - Manage billing account(no create)
- Billing Account User/Project Billing Manager - link projects to billing accounts
- Billing Account Viewer - view billing account details

Data Security
	- Encryption at flight or at Rest
	- Limit Blast Radius 
		- VPC Service Controls - creating a perimeter which can host all the cloud services. these cloud services cannot communicate to other cloud services outside the perimeter
	- GCP Security Command Center 
		- Asset Management
		- Web Security Scanner
		- Anomaly Detection
		- Treath Detection
	- Key Management - 
		- Customer Managed Encryption Keys(CMEK) - Data is encrypted with Data Encryption Key(DEK) which is encrypted by Key Encryption key(KEK) -Cloud Key Management Service (KMS)
		- Customer Supplied Encryption Keys(CSEK) - Keys are kept in Cloud and supplied by Customer
		- Client-Side Encryption  - Keys are supplied by Customer and not kept in cloud
Data Privacy
	- Cloud Data Loss Preventive - to identify Sensitive information and de-identify them by masking,tokenizing, data shifting
		- Text
		- Images
		- Psuedo-Anonymization
		- Risk Analysis

Industry Regulation
	- FedRAMP - Federal Risk and Authorization Management Program - 
	- Children's Online Privacy Protection Act(COPPA) - Parents Consent
	- HIPAA - Health insurance Portability and Accountability Act
	- PCI DSS - Payment Details
	- GDPR(General Data Protection Regulation) - EU 

-----------------------------------------------------------------------------------------------------------------------------------------

Compute
-------------
- Compute Engine - VM's
- App Engine - Can run application Code
- Kubernities Engine - Run Containers
- Cloud Function - Logic for an event
- Cloud Dataproc - can run Apache Spark/hadoop jobs
- Cloud Pub/Sib - Publish and Subscript for events.
- Cloud DataFlow - Apache Beam jobs - support both batch and streaming data 
- Cloud Data Fusion - Build Pipeline by drag and drop and can run in Cloud Dataproc
- Cloud Monitoring - can be used to monitor any process and create alerts and dashboards
- Cloud Composer - Orchestrator with DAGs - Apache Air Flow
- Cloud Dataprep - Data cleanup using recipes and deploy using dataflow

---------------
Compute Engine:
----------------
 - Standard VM's - can spin up by specifying region and zones, when required has some storage on cluster
 - Premptible VM's - can be used for quick computing, no cluster store
 - Persistance Disks - to create storage space with X storage
	Data persist even if the VM is shutdown 
	Data is restricted to the Zone it exist so any VM which is in other region or zone cant access this.
	Snapshot are avaiable globally and can be used to create disk in other zones as needed
	SSD - has faster IOP
	increase of storage space - increases the IOP

---------------
Cloud DataProc
--------------
- Managed Cluster service
- to submit existing Apachi Spark or Hadoop jobs  - ease of scaling, connector to GCS rather than hdfs, other connectors for analytics BQ
- take care of setting up VM, softwares it requires, setting up Master Node(name node) and worker Node(data node) with other hadoop softwares 
- billed per usage per second with min of 1 minute
- Create custom Dataproc image that fulfils the customer requirements and use it to deploy a Dataproc cluster.
- region - regional - global(not regional)
- Cluster choice
	- Single Node cluster - 1 - master and worker nodes in a single machine
	- Standard Cluster- 1- master node(Yarn Resource Manager), multiple worker nodes(Yarn Node Manager+HDFS) - can add preemtible cluster(no HDFS) for more resource
	- High Availability Cluster - 3 Masters, multiple worker nodes
- Clusters	
	- Persistent Clusters - Persistent data cluster
	- Ephemeral Clusters - non Persistent Data Cluster - Prefer to use them 
- Stackdriver loggering - for monitoring the health
- On-permis - HDFS to Cloud Storage 
- Seperate Compute from storage 
	- Run hadoop jobs on Cluster with memory block size - inefficient - old way - On-permis
	- Run Hadoop jobs on Dataproc with Data from Cloud Storage - Petabite bandwidth
- Googles Data Center
	- Store - Colossus
	- Network - Jupiter
- local HDFS is better if it is small chuncks of data, lot many change in the data.
- Cloud Storage for input and output.
- Autoscaling is reactive - based on cluster utilization - configured using yaml file
- Should not use autoscaling when cluster choice - high availability cluster, HDFS,streaming 
- Cloud DataProc workflow templates for scheduling the jobs
- Support MapReduce,Spark,Hive,Pig
- Cloud Storage Connector - for data rather than HDFS and preemptable cluster can be used 
- Other connectors - BigTable, BigQuery, GCS, Stackdriver Logging and Stackdriver Monitoring

How to use
- Data proc - create cluster - region/cluster choice
	 -  jobs - submit job - type of language, main class, arguments, jar file
- job logs persist even if the cluster is deleted
- no-address/network flag for creating internal IP subnet

----------------
Cloud Dataflow - Apache Beam
----------------
- Fully Managed, serverless
- based on Apache Beam = Batch + Stream
- same code for can be used for Batch and stream processing of data 
- new datapipeline - build for future
- Contains - Parallel execution
  - Driver Program - written by java or python
    - pipeline
	- pCollection - immutable data(cannot be updated) - copies of data are made between transformations - Access to all elements - Timestamp for each element
		- Unbounded pCollection - Streaming Data
		- bounded pCollection - Batch Data
	- pTransform - Input/Transformations/output - Map,FlatMap,ParDo,GroupByKey,coGroupByKey,Combine,CombineFn,Flatten,Partition
  - prunners - compute power - local/cloud for running driver program
- Contains the data in pCollections as byte stream(elements) rather than fixed datatypes
- with pCollections and pTransforms a dataflow pipelines is non linear DAG(Direct Acyclic graph - no cycles (output of transform be input of the same transform)
- eventtime(dataelement occurs timestamp on it), processing times(different time when it gets processed = eventime +x , eventtime + y)
- AutoScaling - AutoBalancing in between the process - Auto Optimization once the pipeline is submitted. If there are independent transforms, they would process parallelly
- Intellegent Watermark - is the lag time the window need to wait for getting all the data based on the event timestamp to fall into the window.
- Late Data - data that arrives with timestamp that is inside the window but comes past the watermark
- Side Inputs
- Triggers - when the Beams decides to output aggregated results - assumes all the data is present in the window - for bounded data when it process all data in the window - for unbounded when the watermark passes the end of the window
	- eventtime - default
	- processing time
	- Data-driven 
	- composite 
- Windows - converts streaming data in pCollection to finite windows for aggregate functions
	- Fixed Time window - constant duration - non overlap
	- Sliding windows - overlap - running averages of data(1 min window every 10 sec)
	- Session Window - per key bases - irregularly distributed - each window is seperated by irregular time slot
	- single global window - default
- data skewing - holding up a resource for other resources to complete the work
- Drain Feature - stops pulling the data from the source and tries to complete the dataflow with the existing data.
- maxNumWorkers - to limit scaling 

pTransforms - core beam transforms - https://beam.apache.org/documentation/programming-guide/#core-beam-transforms
- ParDo - acts on a single data from input pCollection and generates zero,one,many data into the output pCollection - DoFn is a template to create userdefined functions referred in ParDo - Map phase in Mapreduce
- Aggregation - to merge common keys or window from an input pCollection and write it to output pCollection
- GroupByKey - Collect all the values associated with a unique key - suffle phase in mapreduce
- CoGroupByKey - Relational join of multiple pCollections with same key type
- Combine - Combining of elements - Min/Max/Sum
- Flatten - Merges multiple pCollections into a single pCollection
- Partition - How the elements of pCollection can be splitup - reduce phase in mapreduce

Flow -
In Cloud - pipeline is triggered in GCP Cloud Dataflow service - which submits GCP Cloud Dataflow jobs - which invokes multiple worker nodes  - which uses cloud storage for data

IAM 
- Cloud Dataflow service accounts - to access VM's and data
- Controller service accounts - worker jobs - compute engine service accounts

Regional Endpoints - to control dataflow works to be availble in the region - reduce latency
Migrating MapReduce jobs to Cloud Dataflow

Cloud Dataflow SQL - Apache Beam SQL - QUery is converted to SQL Transformation in the pipeline for batch or streaming data
How to use
- run pipelines in local to test and debug - remove DataFlowRunner from pipeline options to run locally
- run pipelines in PROD using managed cloud resources

---------------
Cloud Pub/Sub
-------------- 
- Pub/Sub is similar to Kafka with difference of
	- Kafka - Unmanaged, only polling(pull) subscription,Guarantee's order of message in a partition, can manage message retention time, producers-consumers
	- Pub/Sub - Fully Managed serverless, push and pull, no guarantee of order of messages, (10min -7 days) retention, Publishers - subscribers
- Components
	- Publisher - publishes messages based on Topics created
	- Subscriber - Subscribes to the Topics
- one to many/one to one/many to one mapping between publisher -> Topic -> Subscription -> Subscriber
- Used for loosly coupling the systems.
- Request Types
	- Pull - Subscribers initiaties the actions, requests messages, pub sends the message, sub acknowledges the message - default
	- Push - Publisher pushes the message to Subscriber, Sub Acknowledges the message - Https URL with valid SSL Certificate to accept POST requests.
- Acknowledge the message is key as the next messages from the subscriptions will not be passed on until the prior message is acknowledged
- once the message is recieved and acknowledges the message doesnt exist in the topics - default(can make them retain by setting retain ack message to true)
- Pub/Sub guarantee's atlest one message to every subscriber
- Message created in a topic before subscription creation, subscriber will not get prior messages.
- Messages created in a topic without any subscriptions will not have a destination.
- Subscription retention period without activity - 31 days - 365 days or no expiry
- Message are re-sent to Subscriber if there is delay with acknowledgement time.
- Seek Feature - once messages are retained in Topic, we can use seek feature to go to a point of time(backforward/forward) for the subscriber to retrieve all the messages
- snapshot allowed in topic
- Stackdriver logging for checking the health of the pub/sub
- Issues
	- Messages can arrive not in order of the event - use alternative transformations by adding timestamp to the metadata of the message and order them
	- Duplication of message expected
	- Publisher stores for only 7 days.
	- each message has a limitation of 10MB and stored in binary order
- Usecases
	- Distributing Workloads 
	- Asynchronous Workflows
	- Distributing Event Notifications
	- Distributed logging
	- Device Data Streaming
- IAM - grant access to specific topics and subscriptions

How to use
- Create a Topic in Cloud Pub/Sub
- Create a Subscription to the topic created
- Publish a message to the Topic.
- Pull the message from the Published and Acknowledge it so that the Topic is now empty.

Samples:
cloudstorage notifications - pub/sub - cloud function - cloudstorage
cloudstorage - pub/sub - dataflow - bigquery

Commands
gcloud pubsub topics create sandiego
gcloud pubsub topics publish sandiego --message "hello"
messageIds:
- '2201523698972039'
gcloud pubsub subscriptions create --topic sandiego mySub1
Created subscription [projects/qwiklabs-gcp-02-447c7e5c4d12/subscriptions/mySub1].
gcloud pubsub subscriptions pull --auto-ack mySub1
Listed 0 items.
gcloud pubsub topics publish sandiego --message "hello again"
messageIds:
- '2201516210054304'
gcloud pubsub subscriptions pull --auto-ack mySub1
┌─────────────┬──────────────────┬────────────┐
│     DATA    │    MESSAGE_ID    │ ATTRIBUTES │
├─────────────┼──────────────────┼────────────┤
│ hello again │ 2201516210054304 │            │
└─────────────┴──────────────────┴────────────┘
gcloud pubsub subscriptions delete mySub1
gcloud pubsub topics delete sandiego

gcloud pubsub topics list

---------------
Cloud Composer: Apache Airflow
---------------
- Fully Managed Workflow orchestrator built on open source Apache Air Flow to streamline multiple activites.(No vendor lock-in)
- Apache airflow works on DAG and each DAG Contains operators, Cloud Composer manages the resources it need
- Need to create an environment in a GCP Project which contains
	- Cloud Compose environment
	- Kubernities(GKE Cluster)
		- Redis
		- Scheduler
		- worker nodes(min 3)		
	- Pub/Sub - need manually delete after done
	- CloudStorage - to store the DAGS - need to manually delete after done
	- connects to tenet project of google
	- Airflow parameter and Environment variables
- Each of the block of operations are created in DAG(Direct Acyclic Graph) - takes input and outputs but cannot trigger itself
	- a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.
	- written in Python language and placed in DAG folder
	- run this python code to create a DAG object with tasks
	- Each Task with have inbuild operators 
- can be trigger by 
	- Push - Event - using Cloud Functions
	- Pull - Schedule
- monitoring can be done using stackdriver or cloud logging

--------------
Cloud DataPrep - TRIFACTA
--------------
- Exploring, Clearning and preparing Data
- Visualization of Transformation
- Once the Transformation are done, moved to data flow for deployment
- creation of reciepe for all the transformation(by the autosuggestor), use automater to create dataflow

------------------
Cloud DataFusion:
-------------------
- to create visual data pipelines
	Wrangler - for reviewing the data, create transformations and create a reciepie which can be used in datafusion to run the recipie on the whole dataset
	Data Pipeline - Framework - drap and drop 
	Rules Engine - for rules from Business
	Metadata Aggregator - field level details
- Cloud Data Fusion provisions an ephemeral Cloud Dataproc cluster, runs the pipeline
-Lineage of data - transformations which happened on each field of data from source to destination

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Storage
---------
DataLake - Single source of data of the organization - gets data from multiple sources in multiple formats - Datadump - Cloud Storage
Datawarehouse - Processed data which can be used for analytics - Known data - Cloud BigQuery
Cloud Storage - Buckets - files
Cloud SQL - RDMS - single Database/Location - Supports MYSQL/POSTGRES/SQLSERVER, can be single or multiple Zone(Production)
Cloud Spanner - RDMS - Multiple Database/across globe, Horizontal scalability, high avaiability and strong consistency(breaks CAP theorm)
Cloud Datastore/firestore - No SQL Document Database  - entity and fields, schema flexible database, realtime database with mobile SDK's, Strong consistency
Cloud BigTable - High throughput - Sensor data, Optimized for writes
Cloud BigQuery - DataAnalytics with ML SQL, optimized for Reads
Cloud MemoryStore - in-memory store - Managed Redis instances
Cloud Shell - for quering DB's
Data Catalog - Metadata Management Services, doesnt support Datastore/BigTable

--------------  
Cloud Storage -  AWS S3 Bucket
--------------
 - Same zone - low latency
 - Fully Managed Object Storage - objects stored in opaque data(0 and 1)
 - Objects are immutable(cannot be changed)
 - Overwrites are atomic - once the overwrite is complete then only data will be avaiable 
 - Limitation of the object size to 5T
 - version is allowed but is OFF by default
 - Class is respect to object not with respect to version
 - Small data transfers - console, gutil
 - large data transfers - Transfer appliance(one time transfer - TA40 - 40TB or TA300 - 300TB), Transfer services(Source to Sink - scheduled/repeatitive)
 - Can trigger events when new data gets loaded to a bucket

Regional - default - in different zones
Dual-Regional - two regions - same part of the world
Multi-Regional - all data centers - in a geographic region - US/Europe

storage classes - access/availability/latency same only the cost varies
- Standard - Daily or in usage 
- Nearline - Once in a week - 30 days min
- Coldline - Once in a Month - 90 days min
- Archive - High retrival Cost, Low Storage cost - once in a year - 365 days min

Operations Charges
	- Class A Operations - Upload - expensive
	- Class B Operations - Download
	- Free Operations - Delete - free
Network Charges
	- same location access of object(egress) - free
	- egress rates apply for other locations data transfers between GCS
Data Retrieval Charges
	- based on the storage class

Life Cycle Rules 
	- used for deleted or change storage class 
	- driven by rules

IAM - access control - bulk buckets
ACL - Access control List - multiple accounts - granular level
Signed URL - To provide a Cloud storage URL for read/write/delete access and restricting the URL with respect the access time. can only be used with gsutil - No UI Console


-----------
Cloud SQL   - AWS RDB
----------
- Managed Instance(not Fully)
	- need to setup a window for upgrades
	- 30TB limit
- My SQL
	- Primary Instance and Failover Replica in two different zones
	- when Primary Instance failes, the failover takes the load and becomes Primary Instance and a new Failover replica gets created in a different zone - High availability
	- Primary Instance and Read Replica in same zones
     	- no config changes for client applications for Primary Instance  or Failover Replica
- PostgreSQL
	- Primary Instance and Standby Instance, data is stored in Persistance disks
	- when Primary Instance fails, Standby Instance take up load and waits for the Primay Instance to be back online

Work on Cloud SQL
- Create a Cloud SQL Instance - create a Key
- Create a compute engine VM - login to it, download the My SQL client app, provide the IP, Port and Key details
- Connect to the My SQL and import the data/create tables 
- The Stackdriver Logging agent requires the fluentd plugin to be configured to read logs from your database application. 


----------------
Cloud Firestore (replaces Cloud Datastore) - AWS DynomoDB
---------------
- Fully Managed No SQL database - suitable for semi-structured data
- No SQL Document store - horizontal scaling
- Real time DB with mobile SDK
- ACID Transactions - OLTP

- High level structure => Collections => sub collections => documents(JSON files)
- need unique identifier for each document

- Datastore is perfect for semi-structured data less than 1TB in size. Product catalogs are a recommended use case

Example:
info = { name:jack} //json file
db.collection(u'crew').document(u'jimdoc').set(info) //jimdoc is the identifier defined by user
db.collection(u'crew').add(info.to_dict()) // system will create unique identifier

--------------
Cloud Spanner - no AWS service
-------------
- Managed SQL
- Horizontally Scalable
- high levelability 99.999%
- distrupts CAP - Consistency, availability, Partition Tolerance - only two achievable in SQL DB
- to achieve CAP - it sometime sacrifices Availability - 26 sec per month
- Instance creation 
	- Regional or multiregional
	- Number of nodes
 - each instance of a zone in region is replicated and equal number of nodes are present in each zone
- 3 read/write replicas available
- good performance if nodes have below 65% CPU utilization 
- compute workloads in teh same region
- google has specific Configurations -  multiple regions
- locking read-write/read only

Working on Cloud Spanner:
- Create a Cloud Spanner Instance - region/nodes
- open google shell script and run the python scripts to create tables and insert data
- open google console for quering the tables. alters on the tables can't be done usign google console.

-----------------
Cloud Memorystore - AWS ElasticCache
------------------
- Fully Managed Redis instance
- Instance Creation
	- Basic tier/standard tier
	- region
- session storage/in memory data required


--------------
Cloud BigTable - HBase model - https://cloud.google.com/bigtable/docs/overview
--------------- 
- Managed wide column - no SQL Database - key/value
- Very High through put with very less latency and high availability
- 10,000 read/write per node
- fixed access patterns
- time series/Transactional/IOT
- customized for more writes
- BigTable not suitable - data < 300GB or short lived data
- replication - improve row though put but not write.
- write data into the bigtable in batch of near by row-keys
- splitting, merging and rebalancing happen automatically
- Structure
	- Based on Colossus File System to store Data - maintains 3 replicas
	- Uses Clusters with multiple nodes to store metadata of Tablets
	- Colossus uses TABLETS - Datastructure which contains pointers to the data - Bigtable table is sharded into blocks of contiguous rows
- learning system and self optimized and self reliabilie and self redistribution of traffic- Min 300GB required
- Fault Tolerant - if a node is lost, a new nodes come-up and only the metadata and pointers are update not the actual data as it lies is colossus
- Data stored in Tables
	- Rows and Columns (Column Families)
	- Only one index - row key (Unique) 
- converts all the searches and Sorts into Scans
	- Construction of the row key for specific scnearios is key
- increase in nodes have linear relation ship with number of operations can be performed
- Auto-rebalacing of tablets across the nodes will help to get more processing done 
- Visualizer with Heat map to looking for key insights on how the data is being used and for hot spots so that only one node/cluster is not stress much
- Maximum Read throughput - Timestamp first in the row key - data goes to single node - easy access to read
- Maximum Write throughput - Timestamp last in the row key - to remove bottleneck on a single node
- BigTable is created by instance (supports 1000 tables)
	- instance type - production(1 or more cluster - with 3 nodes)/development(single node cluster) - cannot change once created
	- Storage Type - SDD(each node process 2.5TB of data)- always preferred /HDD( 8TB of data per node)
	- App Profile - custom application for connections - single Cluster routing(strong consistency) /multiple Cluster routines
	- Clusters(max 4) - contains nodes
	- Colossus - contains tablets - each node works on the tablet, no two nodes can use a single tablet.
can be accessed by
	- cbt tool - command line for doing different operation 
		- .cbtrc files is required with project and instance details
		- list of instances => cbt listinstances
		- list of cluster => cbt listclusters
		- list of tables => cbt ls
		- list of columns => cbt ls Vechiles
		- retrieve data => cbt read Vechiles
		- create tbale => cbt creatable fires
		- create column families => cbt createfamily fires fwi
	- Hbase shell -  commandline for adminstrative activities - creating/deleting tables -put to insert the data - scan for retreving data 
		- run the quickstart file from google to start the Hbase shell prompt
		- create table => create 'Vechiles', 'loc','det'
		- list table => list
		- description => describe 'Vechiles'
		- Insert data => put ' Vechiles' ,'M111-45','loc:lat','40.1232'
		- retrieve data on key => get 'Vechiles' , 'M111-45'
		- retrieve data => scan 'Vechiles'
				=> scan 'Vechiles' , {COLUMNS => 'det:route', FILTER => "ValueFilter(=,'regexstring:88)"}
				=> scan 'Vechiles',{ROWPREFIXFILER => 'M111-2',COLUMNS =>'loc:lat'}
		
- Load Balancing - This process splits busier/larger tablets in half and merges less-accessed/smaller tablets together, redistributing them between nodes as needed. If a certain tablet gets a spike of traffic, Bigtable splits the tablet in two, then moves one of the new tablets to another node. Bigtable manages all of the splitting, merging, and rebalancing automatically
- Compaction - Bigtable periodically rewrites your tables to remove deleted entries, and to reorganize your data so that reads and writes are more efficient
- reverse
IAM - at project or instance level
Data Storage model
	- Contains row-keys - only one index allowed in bigtables
	- Contains column Families(max 100 colum families per table)
	- each cell contains a timestamp and can store history based on timestamp
   - operations only by row 
   - sparse table system - empty column cells wont waste space in the table
   - garbage collection feature to remove data based on rules

Limits
   - single row key - 4KB 
   - cell limitation - 10MB with history
   - Single row limitation - 100MB
   - SSD clusters: 2.5 TB per node - even though HDD Process more data the speed is low - 10,000 rows per sec(read/write)
   - HDD clusters: 8 TB per node - 500 rows per sec(read)/10,000 write
   - If instance storage utilization reaches 70% per node, additional nodes should be added
   - Bigtable offers optimal latency when the CPU load for a cluster is under 70%. For latency-sensitive applications, however, we recommend that you plan at least 2x capacity for your application's max Bigtable QPS
   - Strong Consistency - Single Cluster. multiple cluster - eventual consistency
   - doesn't perform well when
   	- large number of columns
	- incorrect schema
	- large amount of data
	- scale-up or scale-down - take 20 mins 
	- not enought nodes - 70%
   - Dont use hashing or raw bytes as row keys
   

- Replication increases read through put but reduces write through put as all the cluster needs to be updated with same data.
	- adding of nodes to the cluster - no impact on write through-put
	- adding additional cluster with nodes - impacts write through-put

HDD Outlier than using SSD
- You need to store over 10TB of data.
- You need to maintain costs, Data Archival(more write less read)
- You plan on running batch workloads instead of frequently executing random reads across a small number of rows.

Change from HDD to SSD/Dev to Prod - Export your Bigtable data into a new instance, and configure the new instance type as production with SSDs
- Using one table for all tenants is the most efficient way to store and access multi-tenant data
how to design row-key (avoid hotspots in cluster)
	- think of all the questions you can ask the database
	- field promotion - moving the known or common data into the row key for better querying.
	- the row keys are sorted lexicographically
	- salting
	- samples 
		-Good -  reverse domain names/String Identifier
		- Bad - sequence Numbers, Timestamp at the start of the row-key 
	- reverse timestamp  - subtract of device timestamp with system timestamp - records are sorted with recent to old.
	- use reverse domain 
	- rather than more columns, group all the data in a single column with a delimiter with a unique rowkey ID
for Time series scenarios
	- tall and narrow tables
	- use new rows rather than version
	- logically seperate tables
	- lot many proven schemas available for creating a db
	
---------------
Cloud BigQuery
---------------
- Petabyte scale, serverless, highlyscalable enterprise datawarehouse
- In-memory BI Engine  - for dashboards
	- can use SQL for quering the data 
	- Managed Database(backup, optimization)
- Count() only returns the number but doesn't remove the duplicates
- Groupby is used to remove duplicates
- explore datastudio - for graphs on data, data is cached for one hr - can be set by Data Freshness
- Cloud Dataprep - to get insights on the data
- Normalization(multiple tables to reduce duplication of data) is for RDMS but for BigQuery - Single table as you can have Arrays for columns and STRUCTs (column families) datatype - record
- Follows De-Normalization Process by combining multiple table data into a single table using Structs
- Capacitor Storage  System - Nested and repeated Data - Columns are store seperately and each cell is stored as (values,  repetition level, definition level)
- get granular data from ARRAYs when you need it but not be punished if you don't (BigQuery stores each column individually on disk)
Sample:  Repeat (Datatype)			
1	1234-1	start		01-01-2021
		inprogress	
		complete

Event - Record (Mode) - STRUCT 
	Event.start	- Timestamp 
	Event.end	- Timestamp
 - Import Supported formations - csv/JSON(every records is new line delimited)/Avro(compressed data)/Parquet(encoding small files)/ORC(Optimized Row Columniar - Hive Data)/ Cloud Datastore Export/Cloud Firestore Exports 
Data Transfer service - to get data from Google products/AWS/GCP/socialMedia connectors

Struct Sample:
#standardSQL
SELECT race, participants.name
FROM racing.race_results AS r, r.participants

#standardSQL
SELECT COUNT(p.name) AS racer_count
FROM racing.race_results AS r, UNNEST(r.participants) AS p

- Unnested is used to extract the data from a Record Mode so as to flatten the data.
	You need to UNNEST() arrays to bring the array elements back into rows
	UNNEST() always follows the table name in your FROM clause (think of it conceptually like a pre-joined table)
- Slots 
	- unit of computation Power - number of slots determined by query size and complexity, used for pricing
	- Flat rate pricing to enable a higher total slot quota for your project - fixed pricing available(i.e. x number of slots for y timeperiod)

- Project.dataset.table - bigquery structure
	- Dataset - Similar to Database
		- Native tables - data in teh BQ
		- Enternal Tables - data resides outsite BQ but Schema is present in BQ - Cloud Storage/Cloud BigTable/Google Drive - data lies outside the BQ
			Federated Datasource for bigquery - BigTable/Cloud Storage/Cloud SQL/Google Drive/ not Datastore
		- View(virtual table) - Created by SQL Queries to data in tables and stored in tables of Dataset. They can be queried with SQL. Everytime a SQL query runs on 				view, the underlying queries run for data 
			- control Access to data
			- reduce query complexity
			- Constructing logical tables
			- No Cache of data
			- cannot export data from view
			- no wild card references
			- only 1000 view allowed per dataset
-IAM
	- authentication with IAM - at dataset level
	- to give authentication at record level  - create view on the table and provide required access to view.
	- Cloud DLP - Cloud Data Loss Preventive - to identify Sensitive information and de-identify them by masking,tokenizing, data shifting
	- Data is encrypted by DEK(Data Encryption Key) which is encrypted by Wrapped DEK which is genered by KEK(Key Encryption Key) stored in Cloud Key management service
		-Wrapped DEK and Encrypted data are stored in Big query
Roles
- BigQUery Data Editor - can be assigned to dataset or tables or view - read/update/delete
- BigQuery Data Owner - can be assigned to dataset or tables or view - read/update/delete
- BigQuery Data Viewer - - can be assigned to dataset or tables or view - read
- BigQuery Job User - permission to run the jobs
- BigQuery Metatdata Viewer - view dataset/tables/views metadata

BigQuery Monitoring - Stackdriver monitoring(Admin/Data/SystemEvents Streams) 
	- Cloud Audit logs - BigQueryAuditMetadata

- Optimized for read not good for updates(use BigTable)
- Big Data Transfer Service  -  EL , ELT, ETL
- Partitioning - Save query time
	- Defining Partition on a date/timestamp related column 
	- can also have the flexibility to expire the data 
	- all_sessions_raw_(N) - number of tables partitioned by date
	- Sample with wildcard SELECT * FROM `ecommerce.sales_by_sku_2017*`
	- only 4000 partitions allowed
- Clustering - Saves query time and processing time (Similar to Index) - Saved in a single clusters - reduces latency - only supported via partitioning
	- Defining multiple columns are clusters
	- Order of the columns is critical.
	- Bigquery internally Clusters data automatically
	- no limitation on number of clusters in a table
Sample: CREATE TABLE 'mydataset.mytable' PARTITIONED BY DATE(timestamp) CLUSTERED BY customer_id,product_id

Best Practices
	- Controlled Cost
		- No Select *
		- Use Preview - not cost for preview
		- Limits doesnt help - as all the data is processed to limit the data
		- Streaming inserts costly
		- Use Partitions by date
	- Query Performance
		- Input Data/Data Source
			- Denormalizations
			- less use of wild card tables
			- avoid external sources
		- Shuffling
		- Query Computation
			- No repeated transformation of data
			- Optimize joines
			- Optimize query performance
		- Materialisation
		- SQL Antipatterns
			- Self Joins/unbalanced joins
			- Data Skewing
			- avoid update/insert of single rows
	- Optimising Storage
		- expiration settings
		- long term storage advantages
	- While quering data most of the time consumption is for I/O operations rather than the computes involved in it. That is reducing the query to only pull specific 			Columns information runs faster than the all the columns.
	-Avoid self-joins of large tables
	-Efficient joins - Denormalization
	-Use window functions instead of Joins
	- Join with precomputed values
	-Approximate aggregation functions 
		-  COUNT(DISTINCT …), we can use APPROX_COUNT_DISTINCT
		- APPROX_QUANTILES to compute percentiles
		- APPROX_TOP_COUNT to find the top elements 
		- APPROX_TOP_SUM to compute top elements based on the sum of an element.

BigQuery ML
   Supports ML models
	- Linear Regression
	- Binary Logistic Regression
	- Multi Class Regression
	- K-means clustering
   - Use SQL to train and evaluate
   - Prepare data -> Train -> evaluate -> Predit

- If a query is run, the result is generally cache for 24 hrs in a temporary cache table, If we run the same query there will not be any processing done and query time = 0 Sec
- Compatable with backward filling - filling any missed values which arrive late
- Lineage - To track the source, process and destination of the data - Metadata of the data 
	- Labels can be used for tracking - Key: Value pairs
- BigQuery SQL can solve data consistency problems(5 dimensions of data)
- Cross Join
#standardSQL
SELECT race, participants.name
FROM racing.race_results
CROSS JOIN
race_results.participants # full STRUCT name

#standardSQL
SELECT race, participants.name
FROM racing.race_results AS r, r.participants
- Some Commands
finding the number of elements with ARRAY_LENGTH(<array>)
deduplicating elements with ARRAY_AGG(DISTINCT <field>)
ordering elements with ARRAY_AGG(<field> ORDER BY <field>)
limiting ARRAY_AGG(<field> LIMIT 5)

bq show bigquery-public-data:samples.shakespeare 

bq query --use_legacy_sql=false \
'SELECT
   word,
   SUM(word_count) AS count
 FROM
   `bigquery-public-data`.samples.shakespeare
 WHERE
   word LIKE "%raisin%"
 GROUP BY
   word'

#standardSQL

# copy one day of ecommerce data to explore
CREATE OR REPLACE TABLE ecommerce.revenue_transactions_20170801
#schema
(
  fullVisitorId STRING NOT NULL OPTIONS(description="Unique visitor ID"),
  visitId STRING NOT NULL OPTIONS(description="ID of the session, not unique across all users"),
  channelGrouping STRING NOT NULL OPTIONS(description="Channel e.g. Direct, Organic, Referral..."),
  totalTransactionRevenue FLOAT64 NOT NULL OPTIONS(description="Revenue for the transaction")
)
 OPTIONS(
   description="Revenue transactions for 08/01/2017"
 ) AS
 SELECT DISTINCT
  fullVisitorId,
  CAST(visitId AS STRING) AS visitId,
  channelGrouping,
  totalTransactionRevenue / 1000000 AS totalTransactionRevenue
 FROM `data-to-insights.ecommerce.all_sessions_raw`
 WHERE date = '20170801'
      AND totalTransactionRevenue IS NOT NULL #XX transactions
;

What are ways to overcome stale data? - reflecting of changes when the upstream table has changes
There are two ways to overcome stale data in reporting tables:
- Periodically refresh the permanent tables by re-running queries that insert in new records. This can be done with BigQuery scheduled queries or with a Cloud Dataprep / Cloud Dataflow workflow.
- Use logical views to re-run a stored query each time the view is selected

maxBadRecords - used to set X% of invalid data allowed

#standardSQL
CREATE OR REPLACE VIEW ecommerce.vw_latest_transactions
OPTIONS(
  description="latest 100 ecommerce transactions",
  labels=[('report_type','operational')]
)
AS
SELECT DISTINCT
  date,
  fullVisitorId,
  CAST(visitId AS STRING) AS visitId,
  channelGrouping,
  totalTransactionRevenue / 1000000 AS totalTransactionRevenue
 FROM `data-to-insights.ecommerce.all_sessions_raw`
 WHERE totalTransactionRevenue IS NOT NULL
 ORDER BY date DESC # latest transactions
 LIMIT 100
;

- SAFE_DIVIDE(field1,field2) to avoid divide by 0 errors

-  The difference between a UNION and UNION ALL is that a UNION will not include duplicate records.

- Linking external tables to BigQuery  - Importing data from the google sheet will not create the table but would estabilish a connection. if we update the google sheet. the changes are reflected in the query.

 - STRING_AGG(DISTINCT v2ProductName ORDER BY v2ProductName LIMIT 10) AS products_ordered
	- is used to get sample data from a column with comma seperated.

- ARRAY_AGG(DISTINCT v2ProductName) AS push_all_names_into_array
	- is used to create an array of product names
SELECT
  fullVisitorId,
  date,
  ARRAY_AGG(DISTINCT v2ProductName) AS products_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT v2ProductName)) AS distinct_products_viewed,
  ARRAY_AGG(DISTINCT pageTitle) AS pages_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT pageTitle)) AS distinct_pages_viewed
  FROM `data-to-insights.ecommerce.all_sessions`
WHERE visitId = 1501570398
GROUP BY fullVisitorId, date
ORDER BY date

- Joins
	- Cross join - outerjoin - no need of join condition - merges the tables
	- Inner join - by default - only the common data in both tables
	- Left Inner Join - full data of the left table and matching with the second tables - commonly used join.
	- right Inner join - full data of the right table and matching with the second tables - generaly we will use left inner join by swapping tables.
	
-  The summary will be slightly different because the RAND() component of the WHERE clause will return a different 1% selection of the data each time the query is executed.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ML
----------

Artificial Intelligence - Discipine(Like Physics) - Narrow AI(specific task - Chess) / General AI (broader activites - humans)
	- Machince Learning - one alternative (Like Newton's law)
		- Deep Learning - nueral networks(brain)

ML Activities
- Ingest Data (Transfer Services/CloudStorage)
	- Prepare(Dataproc/DataFlow/Dataprep/Bigquery)
		- PreProcess(DataFlow/DataProc/BigQuery)
			- Discover(AI Hub)
				- Develop(Data Labeling Platform/Deep learning VM Image/AI Platform Notebooks)
					- Train(AI Platform Training/Kubeflow- onpermise)
						- Test&Analysis(TFX Tools)
							- Deploy(AI Platform Predication/Kuberflow-onpremise)
- Models
	- Build a Custom Model - need more data
		- Cloud TPU's
		- Kubernetes Engine
		- Cloud AI Plaftform
			- TensorFlow
			- TensorFlow Extended
			- TPU
			- Kubeflow - to build Machine learning pipelines
		- Big Query ML
		- Cloud DataProc
		- Compute Engine		
	- Build Custom Model (Code Less) - bring your data - reusable Models
		- Cloud AutoML  - transfer learning - using a already trained model for a different usecase to predict a new label with less training data and new feature values 
			- AutoML Vision - enables you to train machine learning models to classify your images according to your own defined labels.
				-AutoML Vision Edge allows you to train and deploy low-latency, high accuracy models optimized for edge devices.To build an application on iOS or Android devices you can use AutoML Vision Edge in ML Kit.
			- AutoML Video Intelligence
			- AutoML Natural Language
			- AutoML Natural translation
			- AutoML Tables
		- Cloud API Hub
	- Pre-trained ML Model - AI Building Blocks
		- Sight
			- Cloud Vision API - identify objects
			- Cloud Video Intelligence API - identify places/objects
		- Language
			- Cloud Translation API - identify languages
			- Cloud Natural Language API - Sentiment Analysis/Counting classification/entity analysis
		- Conversation 
			- Cloud Dialogflow
			- Cloud Text-to-Speech API
			- Cloud Speech-to-Text API
		- Structured Data
			- AutoML Table
			- Cloud Inference API
			- Clound Recommendations API
		- Cloud Jobs API
		- Cloud Data Loss Prevention API
		- Cloud Speech Synthesis API


Vision API
- Modes
	- Synchronous Mode
	- Asynchronous Mode
- Operations
	- Optical Character Recognition(OCR) - Image Text Detection/Document Text Detection
	- Face Detection - Positions of the features of the face (eg. eyes and noise), likelihood of certain emotions, likelihood of headwear
	- Image Property Detection
	- Label Detection
	- Landmark detection - name, log/lat 
	- logo detection -  retail sector	
	- Explicit Content Detection -  Adult/spoof/medical/violance/racy
	- Web Entity/page detection

Video API
- Operations
	- Detect labels
	- Detect Text
	- Detect Explicit content
	- shot change detection
	- Track objects
	- Transcribe speech
 - Google Knowledge Graph search API (MID - Machine unique Id)
 	- Getting a ranked list of the most notable entities that match criteria
	- Predictively Completing  entities within a search box
	- annotating or orgranizing content using knowledge graph  entities

Translaction API
- Neural Machine Translation Model(NMT)
- Phrase based machine translation model (PBMT)

Natural Language API
- Sentiment Analysis - 'score' represents an overall emotional leaning of a test from -1.0 (negative) to 1.0 (positive), and 'magnitude' indicates the overall strength of emotion. 'magnitude' is not normalized so longer text blocks may have greater magnitudes.
- Entity analysis - nouns/things, salience - importance of the entity
- syntax analysis
- entity-sentiment analysis
- content classification

DialogFlow API
- bots
- intent - Classification 

Speech-to-text API
- Synchronous Recognition - returns text after all the input is processed. smaller audio
- Asynchronous Recognition - Poll for the results
- Streaming Recongnition - results are produced while streaming
Speaker Diarization - interpret multiple speaker in the same audio

Text-to-Speech API
- Speech Synthesis Markup Language(SSML) - adding pauses/sounds in the output speech

------------
AutoML API - APIs and Services
------------
- Suit of ML products
- can custom ML models 
- incorporates human labeling service for accurate prediction - label data
- Graphical UI for training/evaluating/predict
- ML Problem is shared with AutoML - Google Neural Network searches for a prefered Model and uses it with transfer learning

Categories
	- Sight with AutoML
		Process
			- Prepare and Manage Images - label data
			- Training Models 
			- Evaludating Model - Metrics - area under the precision/recall curve, Confidence Threshold curve, confusion matrix
			- Deploy Model - for predictions
			- Making Predictions
			- Undeploying Model
		- Auto ML Vision Edge - Export Custom trained model to devices
		- Object Localization feature to detect multiple objects
	- Language with AutoML - to solve domain specific language problems
		- AutoML Natural Language
		- AutoML Translation
	- Structured Data with AutoML
		- AutoML Tables
			- Data Support
			- Automatic Feature Engineering
			- Model Training - parallel testing with different model

------------
BigQuery ML
------------
- Ingest - ML.Weights(inspect)
	- Train - ML.TrainingInfo
		- Evaluate - ML.Evaluate
			- Predict - ML.Predict
- Transform - keyword used in transformations to the raw data before training it and is used to do the same transformations before the prediction
- Labels
- Feature info - ML.Feature_info
- Models
	- Supervised - labeled data
		- Classification - specific class in which label suits with probability
			- linear
			- logistic
			- dnn_classifier
			- boosted_tree_classifier
		- Regression - speicific label values
			- linear_
			- Logistic_reg
			- dnn_regressor
			- boosted_tree_regressor
		- Recommendation
			- tensorflow
			- matrix_factorization
	- UnSupervised - unlabeled data - clusters
		- kmeans
	- Reinforcemen Learning - Contains agents and environemnts, based on a state at a particular points the agent proposes an action and agent is rewarded(+/-) for the action - used in AlphaGo/Chess 
	
	
	
----------
Kubeflow
---------
- ML Toolkit for Kubernetes - to operationalize ML Models by
	- Experimenting in Jupyter Notebooks
	- Training by TensorFlow
	- Model serving and monitoring
- Phases
	- Experimental Phase
		- Problem + Data - Kubeflow pipeline
		- ML Algorithm - Kubeflow (PyTorch/scikit-learn/tensorflow)
		- Experiment - Kubeflow(Jupyter/fairing(for deploying jupyter notebooks))
		- Tune Hyperparameters - Kubeflow katlib
	- Production Phase
		- Transform Data - Kubeflow pipeline
		- Train Model - Kubeflow MPI,PyTorch
		- Serve Model - TFServing,KFserving
		- Monitor -Kubeflow TensorBoard, Metadata

-----------
AI Platform
-----------
- cloud Notebooks
	- for sharing notebooks 
	- Maggic Functions - %%bigquery
- Overview
	- Ingest data - GCS or GCS Transfer service
	- Prepare - Dataflow/Dataproc/BigQuery/Dataprep
	- Preprocess - same as above
	- Discover - AI Hub
	- Develop - Deep Learning VM, AI Plaftform Jupyter notebooks, AI Platform Training, Kubeflow	
	- Train 
	- Test - TensorFlow Extended,AI Platform Prediction, Kubeflow
	- Deploy
- Contains Workers, Master and parameter servers
- Tiers
	- Basic - single instance  - least cose
	- Customer - different configurations
	- Standard_1 and Premium_1 uses multiple worker and parameter servers - costly

		
		
	
- Training Data -> Auto ML -> Predication with REST API


- Kubeflow 
	- Google Opensource ML Pipeline framework
	- can be packaged with dependencies and can run on any cloud provider
	- provides visual flow the actvitives
	

AI Hub - for collaboration of Machine learning models 


Vision - Imports -> Train -> Evaluate -> Validate
	- Need labled data

ML Framework
	- Tensor Flow
	- AI Platform





	
TensorFlow
 - Google Opensource end-to-end ML Platform
 - TensorFlow Lite - Mobile/embedded devices
 - TensorFlow.JS - for browser application
 - TensorFlow Extended- for ML Pipelines
	
Keras
- Open source neural network libraries - written in Python
- High level API for fast experimentation which runs on top of the existing Machine learning frameworks
-tf.kernas - Tensorflow supports keras

Google Colab
- jupiter notebooks with pre existing python libraries

------------------------[
Machine Learning Concepts
-----------------------------
NLP
 - Syntactical Analysis - Grammer
 - Entity Analysis - cateogorizing words - phone numbers, States
 - Sentiment Analysis - Emotional opinion - +ve,-ve, neutral
	- Score -1 to +1
	- Magnitude = 0 to infinity

Methods used to overcome overfitting(fitting of the model so well that the existing label are predicted correctly with the feature but failes to predict unseen data)
 - Data Augmentation - generate more data for training(rotating/flipping) - reduces overfitting and high accuracy in training
 - Regularization
	L1 Regularization - give weights as zero for not important feature, simple - modulus of the weights
	L2 Regularization - decays the weights towards zero but not zero, complex - sum of the squares of the weights
 - DropOut layer - in neural networks where weights are occusionally set to zero 
 - Reduce size of Network - remove layer/units
 - Hold-out - 80-20 principle of Training vs testing data
 - cross validation - K smaller groups(folds) -  1 fold for testing, all k-1 folds for training.. repeat process untill all the K folds are covered
 - feature selection
 - early stop - based on loss graph - stop training the model after few iterations. Dont train the model with high number of iterations which leads to overfitting

Methods use to overcome underfitting


Hyperparameters - values which needs to specified before training the model
	- Model Hyperparameters - relates directly to the model selected  -   Number of Hidden Layers
	- Algorithm Hyperparameters - relates to training the model - Batch Size, Training Epochs(iterations), Regularization, Learning Rate

Data Preparation
 - Dataset Construction
 - Feature Engineering - Biases and weights
 - Missing data - Ignore/Remove/Impute(Mean or Median/Fixed Constant/Most Frequent values/K-Neighbour/DeepLearning)
 - Missing Values 
 	- Missing Completely at Random(MCAR)
	- Missing at Random(MAR)
	- Not Missing at Random(NMAR)
 - Outliners and clipping 
 - One hot Encoding - convert classification data into numberic data for Machine learning
 - Linear Scaling - Transform from bigger range(0-255) to smallar range(0-1)
 - Z-Score - Structure of the data doesn't change but values are moved to O origin
 - LogScaling - small number has many points and vast majority have few points - convering to log(x)
 - Bucketing - moving continous data to discreate data by spliting the data into ranges and selecting a point in each range

Feedforward Neural Network - input- hidden layer - output layer
Recurrent Neural Network(RNN) - Directed cycles - weather forcase model - Time series problem
Convolution Neural Network - Visual learning task/Natural language process - have a convolution layer in the hiddern layers 
	-Pooling Layer - downsamples input - take 2*2 matrix
		- average pooling - Calculate average 
		- Max pooling - maxium value
Generative Adversarial Networks(GANs) - create images/poet/deepfakes
	- Generator network
	- Discriminator network

In Neural Networks
- Wide models are used for Memorization, Deep Modesl are for generalization
- Deep and wide modes are ideal for Recommendation Applications
- Epoch - is a single pass through the training data

Adjustment to Neural Netowrk models
- Biases  - parameter that is adjusted for a neural network to learn from its training data
- weights - Parameters that are to be adjected for a neural network to learning for its traingin data

Model evaluation parameters
- Classification Problem - Confusion matrix, precision, recall, accuracy
- Regression Problem - MAE(Mean Absolute Error) ,MSE(Mean Square Error),RMSE(Root Mean Square Error)

ML Operationalizing 
- Deployment - same pipeline for test and PROD
- Scoring - standard framework - Predictive Model Markup Language(PMML)/Open Neural Network Exchange(ONNX)
- Logging - input request,output response, Model version and data validation error, response time
- Monitoring - Processing/predictive performance
- Retraining - Data drift/concept drift

Example:

Model Creations:
CREATE OR REPLACE MODEL `ecommerce.classification_model`
OPTIONS
(
model_type='logistic_reg',
labels = ['will_buy_on_return_visit']
)
AS
<<Select query data>>


Evaluate: - area under the curve
SELECT
  roc_auc,
  CASE
    WHEN roc_auc > .9 THEN 'good'
    WHEN roc_auc > .8 THEN 'fair'
    WHEN roc_auc > .7 THEN 'decent'
    WHEN roc_auc > .6 THEN 'not great'
  ELSE 'poor' END AS model_quality
FROM
  ML.EVALUATE(MODEL ecommerce.classification_model,  ( <<select query data>>)

evaulation params - better model
roc_auc - more value 
SQRT - less value
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

Streaming Data processing:
-------------------------------
- Continous Stream of data, Difficult to handle aggregate functions
- Data consists of 	
	ELEMENT DTS
- DTS is a event Timestamp setup buy pubsub
- Solution for streaming data - WINDOWS
	- Fixed Window - 60 Sec - new window starts every 60 sec
	- sliding Window - (60,5) - New window starts for every 5 secs with a window span of 60 secs
	- Session Window  - (10*60) - session max size - 600 sec
- windows gets flushed out or customized behaviour after the timeout
- Handles latency of data - late data by using Intellegent watermark
	lag time = actual time - expected time.
	each window is extended to end at windowtime + lag time to accomidate the late data
	- WaterMark options
		- AfterWatermark - based on event time triggered (DTS)
		- AfterProcessTime - based on processing time
		- AfterCount - based on Data driven trigger.
- Streaming Data into Bigquery
	- Uses Streaming Buffer - there is a quote and price for loading Streaming data into Bigquery
	- prefer to use batch or repeated batch which is free
	- DataStudio is used to do visual representation of data.
		- Access to Charts in Datastudio doesn't provide access to the underlying data
		- if Edit access is provided to chart, access is provided to the whole table and rows from which the chart is made.
----------
DataLab - Jupyter notebook
--------
- Creation of VM with Jupyter notebook attached with Persistant disks
- Jupiter notes can be shared easily and stored in Google Cloud Source repository.
- Runs on Python Kernal with in the VM
- can connect to differnt GCP products to display the data.


----------
DataStudio
----------
- helps to generate reports(detailed level) and Dashboards(KPI's and highlevel)
	- both contains charts and tables
- Charts
	- Tables
	- Scorecards - highlevel numbers - KPI's
	- Pie Chart - with % , suitable for less categories
	- Time Series Graphs - X axis - time - trends
	- Bar Chart - discreate numbers
	- Geographical Maps - region trends
	- Area Charts - Cumilative numbers on time series
	- Scatter Plot - size,color of the plot
- For the reports, datastudio gets the information from Cache with a refresh time of 1hr. but if the need to get the data reflected soon, we can use the refresh button so as to query the report with new data from the original source.
- Anthos - open source - on permise or cloud application - development and operations experiences - faster/modernized apps
- cloud source repository - for storing code for free
- Stackdriver for monitoring the activities from logs - who,what and where - Stackdriver audit logs

GIS Functions
---------------
- used to get geo-spacial details
- Bigquery GeoViz for Graphical view of Geo Spacail points in Maps.
- Structs are used to organize columns from Different Tables for more reability.
- With Clauses are used to create named Sub Queries and used for pre-processing of data rather than creating staging tables.
- Lead and Lag Functions to get the previous value of a column for a  specific keys. Need to use partitions and orderby to have the data in sorted order
- RANK function
